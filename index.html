<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="assets/favicon.ico" rel="shortcut icon" />
    <title>
        Watch It Move
    </title>
    <meta content="Watch It Move" property="og:title" />
    <meta content="We discover the appearance and structure of articulated objects by just watching them move from different points of view. We make no assumption about the type of object." name="description" property="og:description" />
    <meta content="https://nvlabs.github.io/whatch-it-move" property="og:url" />

    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat|Segoe+UI" rel="stylesheet" />
</head>

<body>
    <div class="n-header">
    </div>
    <div class="n-title">
        <h1>
            Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    <a href="https://nogu-atsu.github.io/" target="_blank">Atsuhiro Noguchi</a><sup>1, 2</sup>
                </li>
                <li>
                    <a href="http://www.umariqbal.info" target="_blank">Umar Iqbal</a><sup>1</sup>
                </li>
                <li>
                    <a href="https://research.nvidia.com/person/jonathan-tremblay" target="_blank">Jonathan Tremblay</a><sup>1</sup>
                </li>
                <li>
                    <a href="https://www.mi.t.u-tokyo.ac.jp/harada/" target="_blank">Tatsuya Harada</a><sup>2, 3</sup>
                </li>
                <li>
                    <a href="https://oraziogallo.github.io/" target="_blank">Orazio Gallo</a><sup>1</sup>
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    <sup>
                        1
                    </sup>
                    NVIDIA
                </li>
                <li>
                    <sup>
                        2
                    </sup>
                    The University of Tokyo
                </li>
                <li>
                    <sup>
                        3
                    </sup>
                    RIKEN
                </li>
            </ul>
        </div>
    </div>

    <div class="n-article">
        <div class="n-page video">
            <video class="centered" width="100%" autoplay muted loop playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/teaser.mp4#t=0.001" type="video/mp4" />
            </video>
            <div class="videocaption">
                <div>
                    We learn to reconstruct the appearance of an articulated, moving object by "watching" it
                    move in a multi-view video sequence and associated foreground masks, as shown in the animation in
                    (a). Simultaneously, we discover the object's parts and joints with <b>no additional supervision</b>
                    as shown in (b). The learned structure can be used to <b>explicitly</b> re-pose the object, by
                    roto-translating each part around its joint as shown in (c).
                </div>
            </div>
        </div>

        <h2 id="abstract">
            Abstract
        </h2>
        <p>
            Rendering articulated objects while controlling their poses is critical to applications such as
            virtual reality or animation for movies.
            Manipulating the pose of an object, however, requires the understanding of its underlying structure,
            that is, its joints and how they interact with each other.
            Unfortunately, assuming the structure to be known, as existing methods do, precludes the ability to
            work on new object categories.
            We propose to learn both the appearance and the structure of previously unseen articulated objects
            by observing them move from multiple views, <b>with no additional supervision</b>, such as joints
            annotations, or information about the structure.
            Our insight is that adjacent parts that move relative to each other must be connected by a joint.
            To leverage this observation, we model the object parts in 3D as ellipsoids, which allows us to
            identify joints.
            We combine this explicit representation with an implicit one that compensates for the approximation
            introduced.
            We show that our method works for different structures, from quadrupeds, to single-arm robots, to
            humans.
        </p>
        <h2 id="links">
            Links
        </h2>
        <div class="grid download-section">
            <div class="download-thumb">
                <a href="https://arxiv.org/pdf/2112.11347.pdf" target="_blank">
                    <img class="dropshadow" src="assets/paper_preview.png" height=512 />
                </a>
            </div>
            <div class="download-links">
                <ul>
                    <li>
                        <a href="https://arxiv.org/abs/2112.11347" target="_blank">
                            arXiv preprint
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/NVlabs/watch-it-move" target="_blank">
                            Code
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <h2 id="Video">
            Teaser Video
        </h2>
        <p text-align="center">
            <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/oRnnuCVV89o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </p>

        <h2 id="results">
            Results
        </h2>
        <h3 id="reconstruction">
            Reconstruction
        </h3>
        <p>Here we show novel view synthesis results. For a GT frame (left), we reconstruct the appearance (middle), and part segmentation (right) from novel viewpoints.</p>
        <div class="n-page video">
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/reconstruction_5s/baxter.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/reconstruction_5s/nao.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/reconstruction_5s/pandas.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/reconstruction_5s/zju.mp4#t=0.001" type="video/mp4" />
            </video>
        </div>

        <h3 id="repose">
            Re-posing
        </h3>
        <p>Here we show re-posing results. We manually manipulate the pose from a GT frame (left), and render it from novel viewpoints (middle). These poses were never seen in training. We also show the corresponding part segmentation (right).</p>
        <div class="n-page video">
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/repose_8s/atlas.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/repose_8s/cassie.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/repose_8s/iiwa.mp4#t=0.001" type="video/mp4" />
            </video>
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="assets/repose_8s/zju.mp4#t=0.001" type="video/mp4" />
            </video>
        </div>
        <h2 id="citation">
            Citation
        </h2>
        <pre><code>@article{noguchi2021watch,
            title={Watch It Move: {U}nsupervised Discovery of {3D} Joints for Re-Posing of Articulated Objects},
            author={Noguchi, Atsuhiro and Iqbal, Umar and Tremblay, Jonathan and Harada, Tatsuya and Gallo, Orazio},
            journal={arXiv preprint arXiv:2112.11347},
            year={2021}
}</code></pre>

        <h2 id="acknowledgments">
        </h2>
    </div>
    <center>
        <p>
            We adapted the template for this website from <a href="https://nvlabs.github.io/stylegan3/">StyleGAN3</a>.
        </p>
    </center>
</body>

</html>
